{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9cab2f6-12db-44c8-9a32-19ddb70fa3a6",
   "metadata": {},
   "source": [
    "# Running KimiaNet\n",
    "\n",
    "Moving the `KimiaNet_Keras_Feature...` script to a notebook to make my life easier!\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we want to set up some variables and define functions (most of this is from the original code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0bf1d6e-cb70-49fe-81a6-7364ce89d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"combo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed06643-c975-490c-a829-9671c261caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config variables \n",
    "patch_dir = \"./patches/\" + prefix + \"/\"\n",
    "extracted_features_save_adr = \"./extracted_features_\" + prefix + \".pickle\"\n",
    "network_weights_address = \"./weights/KimiaNetKerasWeights.h5\"\n",
    "network_input_patch_width = 250\n",
    "batch_size = 30\n",
    "img_format = 'jpg'\n",
    "use_gpu = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad7c95f4-aedd-4296-b857-68ba42a76f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "\n",
    "if use_gpu:\n",
    "    os.environ['NVIDIA_VISIBLE_DEVICES'] = '0'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "else:\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Lambda\n",
    "# from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.backend import bias_add, constant    \n",
    "\n",
    "import glob, pickle, skimage.io, pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d29ea8-6c37-4aff-a999-2f060dea283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining functions\n",
    "# feature extractor preprocessing function\n",
    "def preprocessing_fn(input_batch, network_input_patch_width):\n",
    "\n",
    "    org_input_size = tf.shape(input_batch)[1]\n",
    "    \n",
    "    # standardization\n",
    "    scaled_input_batch = tf.cast(input_batch, 'float') / 255.\n",
    "    \n",
    "    \n",
    "    # resizing the patches if necessary\n",
    "    resized_input_batch = tf.cond(tf.equal(org_input_size, network_input_patch_width),\n",
    "                                lambda: scaled_input_batch, \n",
    "                                lambda: tf.image.resize(scaled_input_batch, \n",
    "                                                        (network_input_patch_width, network_input_patch_width)))\n",
    "    \n",
    "    \n",
    "    # normalization, this is equal to tf.keras.applications.densenet.preprocess_input()---------------\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    data_format = \"channels_last\"\n",
    "    mean_tensor = constant(-np.array(mean))\n",
    "    standardized_input_batch = bias_add(resized_input_batch, mean_tensor, data_format)\n",
    "    standardized_input_batch /= std\n",
    "    #=================================================================================================\n",
    "    \n",
    "    return standardized_input_batch\n",
    "\n",
    "# feature extractor initialization function\n",
    "def kimianet_feature_extractor(network_input_patch_width, weights_address):\n",
    "    \n",
    "    dnx = DenseNet121(include_top=False, weights=weights_address, \n",
    "                      input_shape=(network_input_patch_width, network_input_patch_width, 3), pooling='avg')\n",
    "\n",
    "    kn_feature_extractor = Model(inputs=dnx.input, outputs=GlobalAveragePooling2D()(dnx.layers[-3].output))\n",
    "    \n",
    "    kn_feature_extractor_seq = Sequential([Lambda(preprocessing_fn, \n",
    "                                                  arguments={'network_input_patch_width': network_input_patch_width}, \n",
    "                                   input_shape=(None, None, 3), dtype=tf.uint8)])\n",
    "    \n",
    "    kn_feature_extractor_seq.add(kn_feature_extractor)\n",
    "    \n",
    "    return kn_feature_extractor_seq\n",
    "\n",
    "# feature extraction function\n",
    "def extract_features(patch_dir, extracted_features_save_adr, network_weights_address, \n",
    "                     network_input_patch_width, batch_size, img_format):\n",
    "        \n",
    "    feature_extractor = kimianet_feature_extractor(network_input_patch_width, network_weights_address)\n",
    "    \n",
    "    patch_adr_list = [pathlib.Path(x) for x in glob.glob(patch_dir+'*.'+img_format)]\n",
    "    feature_dict = {}\n",
    "\n",
    "    for batch_st_ind in tqdm(range(0, len(patch_adr_list), batch_size)):\n",
    "        batch_end_ind = min(batch_st_ind+batch_size, len(patch_adr_list))\n",
    "        batch_patch_adr_list = patch_adr_list[batch_st_ind:batch_end_ind]\n",
    "        patch_batch = np.array([skimage.io.imread(x) for x in batch_patch_adr_list])\n",
    "        batch_features = feature_extractor.predict(patch_batch)\n",
    "        feature_dict.update(dict(zip([x.stem for x in batch_patch_adr_list], list(batch_features))))\n",
    "        \n",
    "        with open(extracted_features_save_adr, 'wb') as output_file:\n",
    "            pickle.dump(feature_dict, output_file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c679b1-5947-4826-8f27-e6729cdbaa39",
   "metadata": {},
   "source": [
    "## Exporting patches to use for KimiaNet input\n",
    "First, we read in the image we want to segment. We also read in the count information output from the Seurat file so we can exclude patches that don't include any transcript info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a7e53d-36d8-4602-9e4d-afe698167edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "853e347a-cd62-47bf-9fd1-1431196c7c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the png image associated with our dataset\n",
    "fname = \"./pngs/\" + prefix + \".png\"\n",
    "full_image = skimage.io.imread(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fbf320d-592e-481c-b485-e2179e3fa8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 36601)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the count info \n",
    "count_data = pd.read_csv('./CountsForSquares_' + prefix + '.txt', sep=\"\\t\")\n",
    "\n",
    "count_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f44c76a9-2ee8-4be2-89c0-75ba30b6d66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296, 36601)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset only patches which have at least one transcript (excludes white space, areas not covered by ST)\n",
    "nonzero = count_data.loc[(count_data.sum(axis=1) != 0),]\n",
    "\n",
    "nonzero.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8023f7e9-37f3-4a71-8adc-d51399c3cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire patch names\n",
    "nonzero_patches = nonzero.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad4a3c04-9e66-4502-8300-0a5612c68494",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_patch_size = 250\n",
    "\n",
    "# iterate over the image's x/y coordinates\n",
    "for x in range(0,full_image.shape[1], output_patch_size):\n",
    "    for y in range(0,full_image.shape[0], output_patch_size):\n",
    "        \n",
    "        # formulate the patch name based on x/y coords\n",
    "        patch_name = '(' + str(x) + ',' + str(y) +')_(' + str(x+output_patch_size) + ',' + str(y+output_patch_size) + ')'\n",
    "        \n",
    "        # we only want to output the patch if it has count info!\n",
    "        if (patch_name in nonzero_patches):\n",
    "        \n",
    "            output_name='patches/' + prefix + '/patch_' + patch_name + '.jpg'\n",
    "\n",
    "            if os.path.exists(output_name):\n",
    "                continue\n",
    "\n",
    "            if x+output_patch_size >= full_image.shape[1] or y+output_patch_size >= full_image.shape[0]:\n",
    "                continue\n",
    "\n",
    "            patch = full_image[x:(x+output_patch_size), y:(y+output_patch_size)]\n",
    "\n",
    "            pil_image=Image.fromarray(patch).convert('RGB')\n",
    "            pil_image.save(output_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c8d6b9-d9fa-4a45-85c1-5115cfcd75ee",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Feature extraction is super simple - only one command!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf63fc90-6327-4a36-95fa-93d459dbff70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:53<00:00,  2.69s/it]\n"
     ]
    }
   ],
   "source": [
    "extract_features(patch_dir, extracted_features_save_adr, network_weights_address, \n",
    "                 network_input_patch_width, batch_size, img_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c51da-b934-478b-a94a-0bcad3c7bfbc",
   "metadata": {},
   "source": [
    "## Convert feature info to a more readable file format\n",
    "Pickle is specific to Python, and I'm doing downstream stuff in R sooo let's convert this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c61c003-9358-40a7-9150-96120197d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "674d3e87-a22b-417d-a1d9-4f00fbc55e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = []\n",
    "with (open(\"./extracted_features_\" + prefix + \".pickle\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            objects.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1027f8dc-021a-42d3-a163-b8663165f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(objects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "940fcae9-d9d0-429e-a9d5-902aee89439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.transpose().to_csv(\"./extracted_features_\" + prefix + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c317a-9b36-484d-95d4-0e3c2436d104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
